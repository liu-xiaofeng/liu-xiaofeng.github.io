<!doctype html>
<html>
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Xiaofeng Liu</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
    </style>



  </head>
  <body>
    <div class="wrapper">
           <header>
        <h1>Xiaofeng Liu</h1>
        <p>Instructor [faculty] <br>Harvard University/MGH</p>
        <p>Gorden Center for Medical Imaging</p>
        <h3><p class="view"><a href="https://liu-xiaofeng.github.io/">Home</a></p></h3>
        <h3><p class="view"><a href="https://liu-xiaofeng.github.io/cv.html">CV</a></p></h3>
        <h3><p class="view"><a href="https://liu-xiaofeng.github.io/research.html">Publications‚Üê</a></p></h3>
        <h3><p class="view"><a href="https://liu-xiaofeng.github.io/projects.html">Projects</a></p></h3>
        <h3><p class="view"><a href="https://liu-xiaofeng.github.io/teaching.html">Teaching/Talks</a></p></h3>
        <h3><p class="view"><a href="https://liu-xiaofeng.github.io/personal.html">Personal</a></p></h3>
    <p class="view"><b>Social</b><br>
        <a href="mailto:liuxiaofengcmu@gmail.com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a><br>
        <a href="https://scholar.google.com/citations?user=VighnTUAAAAJ&hl=en" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Scholar</a><br>
        <a href="https://orcid.org/0000-0002-4514-2016"><i class="ai ai-fw ai-orcid-square"></i> ORCID</a><br>
        <a href="https://github.com/liu-xiaofeng"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>
        <a href="https://twitter.com/liuxiao13718215" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a><br>
        <a href="https://linkedin.com/in/xiaofengcs" class="author-social" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a><br>

    <p><b>Contact:</b><br>Massachusetts General Hospital<br>Harvard Medical School<br>5 Fruit Street, Thier 304<br>Boston, MA 02114<br>liuxiaofengcmu[AT]gmail.com</p>
      </header>
      <section>

    <h2><a id="published-papers-updated" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Selected Published &amp; Forthcoming Papers</h2>

      <h3> Full list @ <a href="https://scholar.google.com/citations?user=VighnTUAAAAJ&hl=en" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Google Scholar</a> </h3>



           <hr>

    <h2>2020</h2>






    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Severity-Aware Semantic Segmentation with Reinforced Wasserstein Training.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Severity-Aware Semantic Segmentation with Reinforced Wasserstein Training.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://liu-xiaofeng.github.io/publications/Severity-Aware Semantic Segmentation with Reinforced Wasserstein Training.pdf">Severity-Aware Semantic Segmentation with Reinforced Wasserstein Training</a></b>
      <br> Xiaofeng Liu, Wenxuan Ji, Jane You, Georges El Fakhri, Jonghye Woo <br> <i>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</i>, 2020.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Severity-Aware Semantic Segmentation with Reinforced Wasserstein Training.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Severity-Aware Semantic Segmentation with Reinforced Wasserstein Training.txt" target="_blank"> [BibTex] </a>
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Semantic segmentation is a class of methods to classify each pixel in an image into semantic classes, which is critical for autonomous vehicles and surgery systems. Cross-entropy (CE) loss-based deep neural networks (DNN) achieved great success w.r.t. the accuracy-based metrics, e.g., mean Intersection-over Union. However, the CE loss has a limitation in that it ignores varying degrees of severity of pair-wise misclassified results. For instance, classifying a car into the road is much more terrible than recognizing it as a bus. To sidestep this, in this work, we propose to incorporate the severity-aware inter-class correlation into our Wasserstein training framework by configuring its ground distance matrix. In addition, our method can adaptively learn the ground metric in a high-fidelity simulator, following a reinforcement alternative optimization scheme. We evaluate our method using the CARLA simulator with the Deeplab backbone, demonstraing that our method significantly improves the survival time in the CARLA simulator. Also, our method can be readily applied to existing DNN architectures and algorithms while yielding superior performance. We report results from experiments carried out with the CamVid and Cityscapes datasets. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>

    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Image2Audio Facilitating Semi-supervised Audio Emotion Recognition with Facial Expression Image.pdf"><img src="https://Image2Audio Facilitating Semi-supervised Audio Emotion Recognition with Facial Expression Image.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://liu-xiaofeng.github.io/publications/Image2Audio Facilitating Semi-supervised Audio Emotion Recognition with Facial Expression Image.pdf">Image2Audio: Facilitating Semi-supervised Audio Emotion Recognition with Facial Expression Image</a></b>
      <br> Gewen He*, Xiaofeng Liu*, Fangfang Fan, Jane You (*contribute equally)<br> <i>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</i>, 2020 workshops.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Image2Audio Facilitating Semi-supervised Audio Emotion Recognition with Facial Expression Image.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Image2Audio Facilitating Semi-supervised Audio Emotion Recognition with Facial Expression Image.txt" target="_blank"> [BibTex] </a>
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Semantic segmentation is a class of methods to classify each pixel in an image into semantic classes, which is critical for autonomous vehicles and surgery systems. Cross-entropy (CE) loss-based deep neural networks (DNN) achieved great success w.r.t. the accuracy-based metrics, e.g., mean Intersection-over Union. However, the CE loss has a limitation in that it ignores varying degrees of severity of pair-wise misclassified results. For instance, classifying a car into the road is much more terrible than recognizing it as a bus. To sidestep this, in this work, we propose to incorporate the severity-aware inter-class correlation into our Wasserstein training framework by configuring its ground distance matrix. In addition, our method can adaptively learn the ground metric in a high-fidelity simulator, following a reinforcement alternative optimization scheme. We evaluate our method using the CARLA simulator with the Deeplab backbone, demonstraing that our method significantly improves the survival time in the CARLA simulator. Also, our method can be readily applied to existing DNN architectures and algorithms while yielding superior performance. We report results from experiments carried out with the CamVid and Cityscapes datasets. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>



    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Importance-Aware Semantic Segmentation in Self-Driving with Discrete Wasserstein Training.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Importance-Aware Semantic Segmentation in Self-Driving with Discrete Wasserstein Training.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://liu-xiaofeng.github.io/publications/Importance-Aware Semantic Segmentation in Self-Driving with Discrete Wasserstein Training.pdf">Importance-Aware Semantic Segmentation in Self-Driving with Discrete Wasserstein Training</a></b>
      <br> Xiaofeng Liu, Yuzhuo Han, Song Bai, Yi Ge, Tianxing Wang, Xu Han, Site Li, Jane You, Jun Lu <br> <i>Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</i>, 2020.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Importance-Aware Semantic Segmentation in Self-Driving with Discrete Wasserstein Training.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Importance-Aware Semantic Segmentation in Self-Driving with Discrete Wasserstein Training.txt" target="_blank"> [BibTex] </a>
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Semantic segmentation (SS) is an important perception manner for self-driving cars and robotics, which classifies each pixel into a pre-determined class. The widely-used cross-entropy (CE) loss-based deep networks has achieved significant progress w.r.t. the mean Intersection-over Union (mIoU). However, the cross-entropy loss can not take the different importance of each class in an self-driving system into account. For example, pedestrians in the image should be much more important than the surrounding buildings when make a decisions in the driving, so their segmentation results are expected to be as accurate as possible. In this paper, we propose to incorporate the importance-aware inter-class correlation in a Wasserstein training framework by configuring its ground distance matrix. The ground distance matrix can be pre-defined following a priori in a specific task, and the previous importance-ignored methods can be the particular cases. From an optimization perspective, we also extend our ground metric to a linear, convex or concave increasing function $w.r.t.$ pre-defined ground distance. We evaluate our method on CamVid and Cityscapes datasets with different backbones (SegNet, ENet, FCN and Deeplab) in a plug and play fashion. In our extenssive experiments, Wasserstein loss demonstrates superior segmentation performance on the predefined critical classes for safe-driving. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>


    <table border="0">
    <tbody><tr>
  <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Unimodal Regularized Neuron Stick-breaking for Ordinal Classification.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Unimodal Regularized Neuron Stick-breaking for Ordinal Classification.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220300618">Unimodal Regularized Neuron Stick-breaking for Ordinal Classification</a></b>
      <br> Xiaofeng Liu, Fangfang Fan, Lingsheng Kong, Zhihui Diao, Wanqing Xie, Jun Lu, Jane You <br> <i>Neurocomputing</i>, 2020, Volume 388, Pages 34-44.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Unimodal Regularized Neuron Stick-breaking for Ordinal Classification.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Unimodal Regularized Neuron Stick-breaking for Ordinal Classification.txt" target="_blank"> [BibTex] </a>
       <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> This paper targets for the ordinal regression/classification, which objective is to learn a rule to predict labels from a discrete but ordered set. For instance, the classification for medical diagnosis usually involves inherently ordered labels corresponding to the level of health risk. Previous multi-task classifiers on ordinal data often use several binary classification branches to compute a series of cumulative probabilities. However, these cumulative probabilities are not guaranteed to be monotonically decreasing. It also introduces a large number of hyper-parameters to be fine-tuned manually. This paper aims to eliminate or at least largely reduce the effects of those problems. We propose a simple yet efficient way to rephrase the output layer of the conventional deep neural network. Besides, in order to alleviate the effects of label noise in ordinal datasets, we propose a unimodal label regularization strategy. It also explicitly encourages the class predictions to distribute on nearby classes of ground truth. We show that our methods lead to the state-of-the-art accuracy on the medical diagnose task (e.g., Diabetic Retinopathy and Ultrasound Breast dataset) as well as the face age prediction (e.g., Adience face and MORPH Album II) with very little additional cost. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>




    <table border="0">
    <tbody><tr>
     <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Dependency-aware Attention Control for Image Set-based Face Recognition.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Dependency-aware Attention Control for Image Set-based Face Recognition.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://ieeexplore.ieee.org/abstract/document/8820094">Dependency-aware Attention Control for Image Set-based Face Recognition</a></b>
      <br> Xiaofeng Liu, Zhenhua Guo, Jane You, B.V.K Kumar  <br> <i>IEEE Transactions on Information Forensics & Security (T-IFS)</i>, 2020, Volume 15, Pages 1501 - 1512.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Dependency-aware Attention Control for Image Set-based Face Recognition.pdf"> [PDF] </a>
     <a href="https://liu-xiaofeng.github.io/bibtex/Dependency-aware Attention Control for Image Set-based Face Recognition.txt" target="_blank"> [BibTex] </a>
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> This paper considers the problem of image set-based face verification and identification. Unlike traditional single sample (an image or a video) setting, this situation assumes the availability of a set of heterogeneous collection of orderless images and videos. The samples can be taken at different check points, different identity documents $etc$ . The importance of each image is usually considered either equal or based on a quality assessment of that image independent of other images and/or videos in that image set. How to model the relationship of orderless images within a set remains a challenge. We address this problem by formulating it as a Markov Decision Process (MDP) in a latent space. Specifically, we first propose a dependency-aware attention control (DAC) network, which uses actor-critic reinforcement learning for attention decision of each image to exploit the correlations among the unordered images. An off-policy experience replay is introduced to speed up the learning process. Moreover, the DAC is combined with a temporal model for videos using divide and conquer strategies. We also introduce a pose-guided representation (PGR) scheme that can further boost the performance at extreme poses. We propose a parameter-free PGR without the need for training as well as a novel metric learning-based PGR for pose alignment without the need for pose detection in testing stage. Extensive evaluations on IJB-A/B/C, YTF, Celebrity-1000 datasets demonstrate that our method outperforms many state-of-art approaches on the set-based as well as video-based face recognition databases. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>


    <table border="0">
    <tbody><tr>
     <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Towards Disentangled Representations for Human Retargeting by Multi-view Learning.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Towards Disentangled Representations for Human Retargeting by Multi-view Learning.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://arxiv.org/pdf/1912.06265">Towards Disentangled Representations for Human Retargeting by Multi-view Learning</a></b>
      <br> Chao Yang, Xiaofeng Liu, Qingming Tang, C-C Jay Kuo <br> <i>Submitted to IEEE T-MM</i>, arXiv:1912.06265.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Towards Disentangled Representations for Human Retargeting by Multi-view Learning.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Towards Disentangled Representations for Human Retargeting by Multi-view Learning.txt" target="_blank"> [BibTex] </a>
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> We study the problem of learning disentangled representations for data across multiple domains and its applications in human retargeting. Our goal is to map an input image to an identity-invariant latent representation that captures intrinsic factors such as expressions and poses. To this end, we present a novel multi-view learning approach that leverages various data sources such as images, keypoints, and poses. Our model consists of multiple id-conditioned VAEs for different views of the data. During training, we encourage the latent embeddings to be consistent across these views. Our observation is that auxiliary data like keypoints and poses contain critical, id-agnostic semantic information, and it is easier to train a disentangling CVAE on these simpler views to separate such semantics from other id-specific attributes. We show that training multi-view CVAEs and encourage latent-consistency guides the image encoding to preserve the semantics of expressions and poses, leading to improved disentangled representations and better human retargeting results. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>


    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Deep Verifier Networks.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Deep Verifier Networks.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://arxiv.org/abs/1911.07421">Deep Verifier Networks: Verification of Deep Discriminative Models with Deep Generative Models</a></b>
      <br> Tong Che*, Xiaofeng Liu*, Site Li, Yubin Ge, Ruixiang Zhang, Caiming Xiong, Yoshua Bengio (*Equally) <br> <i>Submitted to ICML 2020,</i> arXiv:1911.07421.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Deep Verifier Networks.pdf"> [PDF] </a>
     <a href="https://liu-xiaofeng.github.io/bibtex/Deep Verifier Networks Verification of Deep Discriminative Models with Deep Generative Models.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> AI Safety is a major concern in many deep learning applications such as autonomous driving. Given a trained deep learning model, an important natural problem is how to reliably verify the model's prediction. In this paper, we propose a novel framework --- deep verifier networks (DVN) to verify the inputs and outputs of deep discriminative models with deep generative models. Our proposed model is based on conditional variational auto-encoders with disentanglement constraints. We give both intuitive and theoretical justifications of the model. Our verifier network is trained independently with the prediction model, which eliminates the need of retraining the verifier network for a new model. We test the verifier network on out-of-distribution detection and adversarial example detection problems, as well as anomaly detection problems in structured prediction tasks such as image caption generation. We achieve state-of-the-art results in all of these problems. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>





    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Light R-CNN.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/lightrcnn.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="">Light R-CNN: A simple but eÔ¨Écient R-CNN based on reverse residual model</a></b>
      <br> Peng Ding, Arjan Kuijper, Shihua Huang, Xiaofeng Liu, Ping Jia <br> <i>Submitted to Neural Network</i>.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Light R-CNN.pdf"> [PDF] </a>
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">  </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>







  <hr>
    <h2>2019</h2>




         <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Permutation-invariant Feature Restructuring for Correlation-aware Image Set-based Recognition.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Permutation-invariant Feature Restructuring for Correlation-aware Image Set-based Recognition.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Permutation-Invariant_Feature_Restructuring_for_Correlation-Aware_Image_Set-Based_Recognition_ICCV_2019_paper.html">Permutation-invariant Feature Restructuring for Correlation-aware Image Set-based Recognition</a></b>
      <br> Xiaofeng Liu, Zhenhua Guo, Site Li, Lingsheng Kong, Jia Ping, Jane You, Kumar B.V.K <br> <i>International Conference on Computer Vision (ICCV)</i>, 2019.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Permutation-invariant Feature Restructuring for Correlation-aware Image Set-based Recognition.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Permutation-invariant Feature Restructuring for Correlation-aware Image Set-based Recognition.txt" target="_blank"> [BibTex] </a>
       <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">  We consider the problem of comparing the similarity of image sets with variable-quantity, quality and un-ordered heterogeneous images. We use feature restructuring to exploit the correlations of both inner&inter-set images. Specifically, the residual self-attention can effectively restructure the features using the other features within a set to emphasize the discriminative images and eliminate the redundancy. Then, a sparse/collaborative learning-based dependency-guided representation scheme reconstructs the probe features conditional to the gallery features in order to adaptively align the two sets. This enables our framework to be compatible with both verification and open-set identification. We show that the parametric self-attention network and non-parametric dictionary learning can be trained end-to-end by a unified alternative optimization scheme, and that the full framework is permutation-invariant. In the numerical experiments we conducted, our method achieves top performance on competitive image set/video-based face recognition and person re-identification benchmarks. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>


         <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Conservative Wasserstein Training for Pose Estimation.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Conservative Wasserstein Training for Pose Estimation.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Conservative_Wasserstein_Training_for_Pose_Estimation_ICCV_2019_paper.html">Conservative Wasserstein Training for Pose Estimation</a></b>
      <br> Xiaofeng Liu, Yang Zou, Tong Che, Peng Ding, Ping Jia, Jane You, Kumar B.V.K <br> <i>International Conference on Computer Vision (ICCV) </i>, 2019.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Conservative Wasserstein Training for Pose Estimation.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Conservative Wasserstein Training for Pose Estimation.txt" target="_blank"> [BibTex] </a>
       <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">  This paper targets the task with discrete and periodic class labels (eg, pose/orientation estimation) in the context of deep learning. The commonly used cross-entropy or regression loss is not well matched to this problem as they ignore the periodic nature of the labels and the class similarity, or assume labels are continuous value. We propose to incorporate inter-class correlations in a Wasserstein training framework by pre-defining (ie, using arc length of a circle) or adaptively learning the ground metric. We extend the ground metric as a linear, convex or concave increasing function wrt arc length from an optimization perspective. We also propose to construct the conservative target labels which model the inlier and outlier noises using a wrapped unimodal-uniform mixture distribution. Unlike the one-hot setting, the conservative label makes the computation of Wasserstein distance more challenging. We systematically conclude the practical closed-form solution of Wasserstein distance for pose data with either one-hot or conservative target label. We evaluate our method on head, body, vehicle and 3D object pose benchmarks with exhaustive ablation studies. The Wasserstein loss obtaining superior performance over the current methods, especially using convex mapping function for ground metric, conservative label, and closed-form solution. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>


         <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Confidence Regularized Self-Training.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Confidence Regularized Self-Training.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="http://openaccess.thecvf.com/content_ICCV_2019/html/Zou_Confidence_Regularized_Self-Training_ICCV_2019_paper.html">Confidence Regularized Self-Training</a></b>
      <br> Yang Zou, Zhiding Yu, Xiaofeng Liu*, Kumar B.V.K., Jinsong Wang, (*corresponding author) <br> <i>International Conference on Computer Vision (ICCV)</i>, 2019.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Confidence Regularized Self-Training.pdf"> [PDF] </a> <br> <a href="https://liu-xiaofeng.github.io/publications/Confidence Regularized Self-Training supply.pdf"> [Supply] </a>
     <a href="https://liu-xiaofeng.github.io/bibtex/Confidence Regularized Self-Training.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Recent advances in domain adaptation show that deep self-training presents a powerful means for unsupervised domain adaptation. These methods often involve an iterative process of predicting on target domain and then taking the confident predictions as pseudo-labels for retraining. However, since pseudo-labels can be noisy, self-training can put overconfident label belief on wrong classes, leading to deviated solutions with propagated errors. To address the problem, we propose a confidence regularized self-training (CRST) framework, formulated as regularized self-training. Our method treats pseudo-labels as continuous latent variables jointly optimized via alternating optimization. We propose two types of confidence regularization: label regularization (LR) and model regularization (MR). CRST-LR generates soft pseudo-labels while CRST-MR encourages the smoothness on network output. Extensive experiments on image classification and semantic segmentation show that CRSTs outperform their non-regularized counterpart with state-of-the-art performance. The code and models of this work are available at https://github.com/yzou2/CRST.  </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>



         <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Unimodal-uniform Constrained Wasserstein Training for Medical Diagnosis.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Unimodal-uniform Constrained Wasserstein Training for Medical Diagnosis.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="http://openaccess.thecvf.com/content_ICCVW_2019/html/VRMI/Liu_Unimodal-Uniform_Constrained_Wasserstein_Training_for_Medical_Diagnosis_ICCVW_2019_paper.html">Unimodal-uniform Constrained Wasserstein Training for Medical Diagnosis</a></b>
      <br> Xiaofeng Liu, Xu Han, Yukai Qiao, Yi Ge, Lu Jun <br> <i>International Conference on Computer Vision (ICCV)</i>, 2019 Workshops.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Unimodal-uniform Constrained Wasserstein Training for Medical Diagnosis.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Unimodal-uniform Constrained Wasserstein Training for Medical Diagnosis.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">  The labels in medical diagnosis task are usually discrete and successively distributed. For example, the Diabetic Retinopathy Diagnosis (DR) involves five health risk levels: no DR (0), mild DR (1), moderate DR (2), severe DR (3) and proliferative DR (4). This labeling system is common for medical disease. Previous methods usually construct a multi-binary-classification task or propose some re-parameter schemes in the output unit. In this paper, we target on this task from the perspective of loss function. More specifically, the Wasserstein distance is utilized as an alternative, explicitly incorporating the inter-class correlations by pre-defining its ground metric. Then, the ground metric which serves as a linear, convex or concave increasing function w.r.t. the Euclidean distance in a line is explored from an optimization perspective. Meanwhile, this paper also proposes of constructing the smoothed target labels that model the inlier and outlier noises by using a unimodal-uniform mixture distribution. Different from the one-hot setting, the smoothed label endues the computation of Wasserstein distance with more challenging features. With either one-hot or smoothed target label, this paper systematically concludes the practical closed-form solution. We evaluate our method on several medical diagnosis tasks (e.g., Diabetic Retinopathy and Ultrasound Breast dataset) and achieve state-of-the-art performance. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>



          <table border="0">
    <tbody><tr>
    <td width="140"><a href="http://information.casip.ac.cn/caspatent/showdetails.xhtml?para=CN201910491484.7&ptype=patent"><img src="https://liu-xiaofeng.github.io/publications/figs/CN110189660A.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="http://information.casip.ac.cn/caspatent/showdetails.xhtml?para=CN201910491484.7&ptype=patent">‰∏ÄÁßçÊ≤âÊµ∏ÂºèÂÖ®ÊôØÊµÆÁ©∫ÊàêÂÉèÁ≥ªÁªüÔºàImmersive panoramic floating imaging systemÔºâ</a></b>
      <br>  <i> CN Patent </i>CN110189660A.

       <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">  Êú¨ÂèëÊòéÂÖ¨ÂºÄ‰∫Ü‰∏ÄÁßçÊ≤âÊµ∏ÂºèÂÖ®ÊôØÊµÆÁ©∫ÊàêÂÉèÁ≥ªÁªüÔºåÂåÖÊã¨ÔºöÁî®‰∫éÂ∞ÜÂæÖËßÇÁúãÂÖ®ÊôØÂõæÂÉèËøõË°åÂàÜÂå∫Â§ÑÁêÜ„ÄÅÂπ∂Â∞ÜÂàÜÂå∫Â§ÑÁêÜ‰πãÂêéÁöÑÂÖ®ÊôØÂõæÂÉèËøõË°åÂàÜÂå∫ÊòæÁ§∫ÁöÑÊòæÁ§∫Ê®°Âùó„ÄÅÁî®‰∫éÂ∞ÜÊòæÁ§∫Ê®°ÂùóÂèëÂá∫ÁöÑÂÖâÁ∫øËøõË°åÂàÜÂÖâÂ§ÑÁêÜÁöÑÂàÜÂÖâÊ®°Âùó„ÄÅ‰ª•ÂèäÁî®‰∫é‰ΩøÂàÜÂÖâÊ®°ÂùóÂèçÂ∞ÑÁöÑÂÖâÁ∫øÂéüË∑ØËøîÂõûÂπ∂Á©øËøáÂàÜÂÖâÊ®°ÂùóÂêéÊ±áËÅöÊàêÂÖ®ÊôØÊµÆÁ©∫ÂõæÂÉèÁöÑÈÄÜÂèçÂ∞ÑÂπïÂ¢ôÔºõÂàÜÂÖâÂ§ÑÁêÜÂåÖÊã¨‰ΩøÊòæÁ§∫Ê®°ÂùóÂèëÂá∫ÁöÑÂÖâÁ∫øÁ©øËøáÂàÜÂÖâÊ®°Âùó„ÄÅ‰ª•ÂèäÂ∞ÜÊòæÁ§∫Ê®°ÂùóÂèëÂá∫ÁöÑÂÖâÁ∫øËøõË°åÂèçÂ∞Ñ„ÄÇÁõ∏ÊØî‰∫éÁé∞ÊúâÊäÄÊúØÔºåÊú¨ÂèëÊòéÊèê‰æõÁöÑÊ≤âÊµ∏ÂºèÂÖ®ÊôØÊµÆÁ©∫ÊàêÂÉèÁ≥ªÁªüÊâÄÂΩ¢ÊàêÁöÑÂÖ®ÊôØÊµÆÁ©∫ÂÆûÂÉèÔºåËßÇÊµãËÄÖÂèØ‰ª•Ë¢´ÊµÆÁ©∫ÂÆûÂÉèÁéØÁªïËøõË°å360¬∞ËßÇÁúãÔºåËÉΩÂ§ü‰∏∫ËßÇÁúãËÄÖÂ∏¶Êù•Ê≤âÊµ∏ÂºèÁöÑÂÖ®ÊôØËßÇÊÑüÔºåÂ∫îÁî®‰∫éËàûÂè∞„ÄÅÂ±ïËßàÁ≠âÂú∫ÊôØÊó∂ÔºåÊµÆÁ©∫ÂÆûÂÉèÁöÑÂ≠òÂú®ÂèØ‰ª•Â∏¶Êù•Â§ßËßÜÂú∫„ÄÅÂ§öÊñπ‰ΩçÁöÑÊ≤âÊµ∏ÂºèËßÇÊÑü„ÄÇ </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>




          <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://zhuanli.tianyancha.com/f75ee42ce1d8b4a28bc93eadb7e879ee"><img src="https://liu-xiaofeng.github.io/publications/figs/CN110139092A.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://zhuanli.tianyancha.com/f75ee42ce1d8b4a28bc93eadb7e879ee">Á´ã‰ΩìÊòæÁ§∫Á≥ªÁªü„ÄÅÂõæÂÉèÂ§ÑÁêÜÊñπÊ≥ï„ÄÅË£ÖÁΩÆ„ÄÅËÆæÂ§áÂèäÂ≠òÂÇ®‰ªãË¥® ÔºàStereoscopic display system, image processing method, device, equipment and storage mediumÔºâ</a></b>
      <br>  <i> CN Patent </i>CN110139092A.
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Êú¨ÂèëÊòéÊèê‰æõ‰∫Ü‰∏ÄÁßçÁ´ã‰ΩìÊòæÁ§∫Á≥ªÁªü„ÄÅÂõæÂÉèÂ§ÑÁêÜÊñπÊ≥ï„ÄÅË£ÖÁΩÆ„ÄÅËÆæÂ§áÂèäÂ≠òÂÇ®‰ªãË¥®ÔºåÂåÖÊã¨Êõ≤Èù¢ÊòæÁ§∫Â±è„ÄÅÊõ≤Èù¢ËßÜÂ∑ÆÂÖâÊ†ÖÂíåÂõæÂÉèÂ§ÑÁêÜÊ®°ÂùóÔºåÊõ≤Èù¢ÊòæÁ§∫Â±èÁî±Ëã•Âπ≤‰∏™‰æùÊ¨°‰ª•ÂàóÊéíÂàóËÆæÁΩÆÁöÑÊòæÁ§∫ÂÉèÁ¥†ÂçïÂÖÉÁªÑÊàêÔºåÊòæÁ§∫ÂÉèÁ¥†ÂçïÂÖÉÂåÖÊã¨‰ª•ÂàóÈ°∫Ê¨°ËÆæÁΩÆÁöÑÁ¨¨‰∏ÄÂÉèÁ¥†„ÄÅÁ¨¨‰∫åÂÉèÁ¥†ÂíåÁ¨¨‰∏âÂÉèÁ¥†ÔºåÁ¨¨‰∏ÄÂÉèÁ¥†Áî®‰∫éÊòæÁ§∫Âè≥ÁúºËßÜÂ∑ÆÂõæÂÉèÔºåÁ¨¨‰∏âÂÉèÁ¥†Áî®‰∫éÊòæÁ§∫Â∑¶ÁúºËßÜÂ∑ÆÂõæÂÉèÔºåÁ¨¨‰∫åÂÉèÁ¥†‰∏∫ÂÖ®ÈªëÂÉèÁ¥†ÔºõÂõæÂÉèÂ§ÑÁêÜÊ®°ÂùóÁî®‰∫éÊ†πÊçÆÂæÖÂ§ÑÁêÜÁöÑÂõæÂÉèÊ∫êÁîüÊàê‰∏éÊõ≤Èù¢ÊòæÁ§∫Â±èÁöÑÊòæÁ§∫ÂÉèÁ¥†ÂçïÂÖÉÁöÑÊéíÂàóÁªìÊûÑÂØπÂ∫îÁöÑËßÜÂ∑ÆÂõæÂÉèÔºõÊõ≤Èù¢ËßÜÂ∑ÆÂÖâÊ†ÖÔºåÁî®‰∫éÊéßÂà∂Êõ≤Èù¢ÊòæÁ§∫Â±è‰∏äÊòæÁ§∫ÁöÑËßÜÂ∑ÆÂõæÂÉèÊâÄÂèëÂá∫ÂÖâÁ∫øÁöÑÂèØÈÄöËøáÂå∫Âüü„ÄÇÊú¨ÊäÄÊúØÊñπÊ°àÈÄöËøáÂ∞ÜÊõ≤Èù¢Â±èÂíåÊõ≤Èù¢ËßÜÂ∑ÆÂÖâÊ†ÖÁõ∏ÁªìÂêàÔºåÊèêÈ´ò‰∫ÜÁ´ã‰ΩìÊòæÁ§∫Á≥ªÁªüÁöÑËá™Áî±Â∫¶Ôºå‰∏∫ËßÇÁúãËÄÖÂ∏¶Êù•Ê≤âÊµ∏ÂºèÁöÑËá™Áî±Á´ã‰ΩìËßÇÊÑü„ÄÇ  </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>



         <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/RESEARCH ON THE TECHNOLOGY OF DEEP LEARNING BASED FACE IMAGE RECOGNITION.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/thesis.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CDFD&dbname=CDFDLAST2019&filename=1019042153.nh&v=Mjk3MzkxRnJDVVI3cWZaT1JvRmkzZ1Y3N0pWRjI2RjdPOEhOREpySkViUElSOGVYMUx1eFlTN0RoMVQzcVRyV00=">Research on the technology of deep learning based face image recognition</a></b>
      <br> Xiaofeng Liu<br> <i>PhD Thesis</i>, University of Chinese Academy of Sciences.
      <br> <a href="https://liu-xiaofeng.github.io/publications/RESEARCH ON THE TECHNOLOGY OF DEEP LEARNING BASED FACE IMAGE RECOGNITION.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/RESEARCH ON THE TECHNOLOGY OF DEEP LEARNING BASED FACE IMAGE RECOGNITION.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> ÈöèÁùÄ‰∫∫Â∑•Êô∫ËÉΩ(Artificial intelligence)ÂíåËÆ°ÁÆóÊú∫ËßÜËßâ(Computer vision)Á≠âÊäÄÊúØÁöÑÈ£ûÈÄüÂèëÂ±ï,ËÆ©Êú∫Âô®ÊàñËÆ°ÁÆóËÆæÂ§áÊã•ÊúâÊÑüÁü•ÂõæÂÉèÂÜÖÂÆπÁöÑËÉΩÂäõÊó•ÁõäÊàê‰∏∫ÂΩì‰ªäÁÉ≠ÁÇπÁ†îÁ©∂ËØæÈ¢ò„ÄÇÂÖ∂‰∏≠‰∫∫ËÑ∏ÂõæÂÉèÂõ†ÂÖ∂‰∏∞ÂØåÁöÑ‰ø°ÊÅØÂíåÂπøÈòîÁöÑÂ∫îÁî®ÂâçÊôØÊõ¥ÊòØ‰∏Ä‰∏™Ê¥ªË∑ÉÁöÑÂàÜÊîØ„ÄÇ‰∫∫ËÑ∏ÂõæÂÉèËØÜÂà´Âè™Ë¶ÅÊòØÊåáÈÄöËøáÂØπ‰∫∫ÁöÑÈù¢ÈÉ®ÂõæÂÉè„ÄÅËßÜÈ¢ëÊàñÂõæÁâáÂíåËßÜÈ¢ëÁöÑÈõÜÂêàËøõË°åÂàÜÊûê,Ëá™Âä®ÁöÑÊé®Êñ≠ÂÖ∂Ë∫´‰ªΩ„ÄÅË°®ÊÉÖ‰ª•ÂèäÂπ¥ÈæÑ„ÄÅÊÄßÂà´Á≠âÂ±ûÊÄß„ÄÇ‰∫∫ËÑ∏ÂõæÁâáÈÄöÂ∏∏Á≥ÖÂêà‰∫ÜÂåÖÊã¨Ë∫´‰ªΩ„ÄÅË°®ÊÉÖ„ÄÅÂπ¥ÈæÑ„ÄÅÊÄßÂà´„ÄÅÂÖâÁÖß„ÄÅËßíÂ∫¶Á≠âÂêÑÁßç‰ø°ÊÅØ,Â¶Ç‰ΩïÊèêÂèñ‰∏éËØÜÂà´‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÁâπÂæÅ(‰æãÂ¶ÇË∫´‰ªΩ‰ø°ÊÅØÂØπÂ∫î‰∫∫ËÑ∏ËØÜÂà´,Ë°®ÊÉÖ‰ø°ÊÅØÂØπÂ∫îË°®ÊÉÖËØÜÂà´)ÊòØÂΩìÂâçÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊ®°ÂºèËØÜÂà´ÁÆóÊ≥ïÁöÑ‰∏ªË¶ÅÁ†îÁ©∂ÊñπÂêë„ÄÇÊú¨ÊñáËøõ‰∏ÄÊ≠•ÁöÑÊé¢Á©∂‰∫ÜÂ¶Ç‰ΩïÂèØÊéßÁöÑÂºïÂÖ•ÂØπÊüê‰∫õ‰∏çÁõ∏ÂÖ≥Â±ûÊÄßÁöÑ‰∏çÂèòÊÄßÂÖàÈ™åÁü•ËØÜ,‰ΩøÂæóÊèêÂèñÂá∫ÁöÑÈíàÂØπ‰∏ªËØÜÂà´‰ªªÂä°ÁöÑÁâπÂæÅÊúâÊõ¥Â•ΩÁöÑÂèØËæ®Âà´ÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ‰æãÂ¶ÇË°®ÊÉÖËØÜÂà´ÁöÑÁâπÂæÅÂ∫îËØ•ÂØπË∫´‰ªΩÂèòÂåñÈ≤ÅÊ£í,ËÄåË∫´‰ªΩËØÜÂà´ÁöÑÁâπÂæÅÂ∫îËØ•ÂØπË°®ÊÉÖ„ÄÅÂÖâÁÖß„ÄÅÂåñÂ¶ÜÁ≠âÂèòÂåñÈ≤ÅÊ£í„ÄÇÈíàÂØπËøô‰∏ÄÁõÆÊ†á‰ª•Âèä‰∫∫ËÑ∏Ë°®ÊÉÖËØÜÂà´Âíå‰∫∫ËÑ∏ËØÜÂà´Ëøô‰∏§‰∏™ÈáçË¶ÅÂ∫îÁî®,Êú¨ÊñáÊèêÂá∫‰∫ÜËá™ÈÄÇÂ∫îÊ∑±Â∫¶Â∫¶ÈáèÂ≠¶‰π†‰ª•ÂèäÂØπÊäóËÆ≠ÁªÉ‰∏§ÁßçÊñπÊ≥ïÁî®‰ª•Ëß£ÊûÑË°®ÊÉÖ‰∏éË∫´‰ªΩ‰ø°ÊÅØÊàñË∫´‰ªΩ‰∏éÊüê‰∫õÂ±ûÊÄß‰ø°ÊÅØ„ÄÇÈöèÁùÄÊï∞Â≠óÂ™í‰ΩìÂÜÖÂÆπÁöÑÁàÜÁÇ∏ÊÄßÂ¢ûÈïø,‰ΩøÁî®ÂõæÁâáÂíåËßÜÈ¢ëÁöÑÈõÜÂêàËøõË°åË∫´‰ªΩËØÜÂà´Êõ¥Á¨¶ÂêàÂÆûÈôÖÁöÑÁîüÁâ©ÁâπÂæÅËØÜÂà´Â∫îÁî®„ÄÇ‰æãÂ¶ÇÊü•ËØ¢ÂõæÁâáÊàñËßÜÈ¢ëÂ∏ßÂèØ‰ª•‰ªéÂ§ö‰∏™ÊëÑÂÉèÂ§¥ÊâÄÊãçÊëÑÂà∞ÁöÑÊüê‰∫∫ÊâÄËé∑Âæó,ËÄåÂÄôÈÄâÂõæÁâáÊàñËßÜÈ¢ëÂ∏ß‰πüÂèØÁî±ËØ•‰∫∫ÂéÜÂè≤‰∏äÂêÑÁßçËØÅ‰ª∂ÁÖßÂíåÂÖ∂‰ªñÂú∫ÂêàÊâÄÊãçÊëÑÁÖßÁâáÁªÑÊàê„ÄÇÁõ∏ËæÉ‰∫é‰º†ÁªüÁöÑÂõæÁâá‰∏éÂõæÁâá,ÊàñËßÜÈ¢ë‰∏éËßÜÈ¢ëÁöÑÁõ∏‰ººÂ∫¶ÂØπÊØî,ËØ•ËÆæÂÆöÂèØ‰ª•Êèê‰æõÊõ¥‰∏∞ÂØåÁöÑ‰ø°ÊÅØ,‰ΩÜ‰πüÁªô‰ø°ÊÅØËûçÂêàÂ∏¶Êù•‰∫ÜÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ¢ûÂº∫Â≠¶‰π†ÁÆóÊ≥ïÁöÑÂõæÁâáÈõÜ‰∫∫ËÑ∏ËØÜÂà´ÊñπÊ°à„ÄÇÊú¨ÊñáÁöÑ‰∏ªË¶ÅË¥°ÁåÆÂèØÊ¶ÇÊã¨‰∏∫:1.‰∫∫Á±ªÁöÑÈù¢ÈÉ®Ë°®ÊÉÖÊòØ‰º†ÈÄíÊÉÖÊÑü‰ø°ÊÅØÁöÑÈáçË¶ÅÈÄîÂæÑ„ÄÇÂü∫‰∫éËá™ÈÄÇÂ∫îÂ∫¶ÈáèÂ≠¶‰π†ÁöÑÂéªÈô§Ë∫´‰ªΩ‰ø°ÊÅØÂπ≤Êâ∞ÁöÑ‰∫∫ËÑ∏(ÂõæÁâá/ËßÜÈ¢ë)Ë°®ÊÉÖËØÜÂà´ÁâπÂæÅÊèêÂèñ„ÄÇÁ¨¨‰∫åÁ´†ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ∑±Â∫¶Â∞∫Â∫¶Â≠¶‰π†ÁÆóÊ≥ï(N+M)ÂÖÉÁªÑÁ∞áÊçüÂ§±,Ëß£ÂÜ≥‰∫ÜËØ•È¢ÜÂüüÈïøÊúüÂ≠òÂú®ÁöÑÈîöÁÇπÈÄâÊã©ÈóÆÈ¢òÂπ∂Â§ßÂ§ßÂáèÂ∞ë‰∫ÜËøêÁÆóÈáè„ÄÇÂÖ∂ÈòàÂÄºÂèÇÊï∞ÂèØËá™ÈÄÇÂ∫îÂ≠¶‰π†„ÄÇÈÄöËøáÂêàÁêÜÁöÑËÆæÁΩÆË¥üÊ†∑Êú¨‰∏∫Âêå‰∏Ä‰∏™‰ΩìÁöÑÂÖ∂‰ªñË°®ÊÉÖÂõæÂÉèËÉΩÊúâÊïàÂú∞ÂÆûÁé∞ÈöæÊ†∑Êú¨ÊåñÊéò,Âπ∂ÊòéÁ°ÆÁöÑÊ∂àÈô§Ë∫´‰ªΩ‰ø°ÊÅØÁöÑÂπ≤Êâ∞„ÄÇÂú®CK+,MMIÂíåSFEWÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÂÖ∂ÂèØ‰ª•ÊúâÊïàÁöÑÂà©Áî®‰∫∫ËÑ∏Ë°®ÊÉÖÊï∞ÊçÆÂ∫ì‰∏≠Â∏∏ÊúâÁöÑË∫´‰ªΩÊ†áËÆ∞‰ªéËÄåÊúâÊïàÂú∞ÊèêÈ´òË°®ÊÉÖËØÜÂà´‰ªªÂä°ÁöÑÁ≤æÂ∫¶„ÄÇ2.ËÄÉËôëÂà∞Êù•Ëá™Âêå‰∏Ä‰∏™‰∏™‰ΩìÁöÑ‰∏≠ÊÄßË°®ÊÉÖÂõæÁâáÊòØËæÉÂ•ΩÁöÑÂèÇËÄÉÂõæÂÉè,‰ΩÜ‰∏çÊòØÊâÄÊúâÊï∞ÊçÆÈõÜ‰∏≠ÈÉΩÊôÆÈÅçÂ≠òÂú®,Á¨¨‰∏âÁ´†ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈöæÊ†∑Êú¨ÁîüÊàêÊñπÊ≥ïÂπ∂ÈÖç‰ª•ÂæÑÂêëÂ∫¶ÈáèÂ≠¶‰π†„ÄÇÈÄöËøáÂ∞ÜÊü•ËØ¢ÂõæÁâá‰∏éÂü∫‰∫éÂÖ∂ÁîüÊàêÁöÑÂêåË∫´‰ªΩ‰∏≠ÊÄßËÑ∏ÂèÇËÄÉÂõæÁâáËøõË°åÊØîËæÉËÄåÂéªÈô§Ë∫´‰ªΩ‰ø°ÊÅØÁöÑÂΩ±Âìç„ÄÇÂÖ∂ÈöæÊ†∑Êú¨ÁîüÊàêÊòØÂü∫‰∫éÂÉèÁ¥†Á∫ßÂØπÊäóÁîüÊàêÁΩëÁªú‰ª•ÂéªÈô§Ë°®ÊÉÖ„ÄÅÂßøÊÄÅÁ≠âÂ±ûÊÄßÂπ≤Êâ∞ÁöÑË∫´‰ªΩ‰∏çÂèòÂΩí‰∏ÄÂåñËÑ∏ÁîüÊàê„ÄÇÈÄöËøáÂú®CK+,MMIÂíåSFEWÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÈöæÊ†∑Êú¨ÁîüÊàêÂèØ‰ª•Âà©Áî®ËøúÂ§ß‰∫éË°®ÊÉÖËØÜÂà´Â∫ìÁöÑ‰∫∫ËÑ∏Ë∫´‰ªΩËØÜÂà´Â∫ì‰∏≠ÁöÑÊ≠£Èù¢‰∏≠ÊÄßÂõæÁâá‰ª•ÂΩ¢ÊàêÂèÇËÄÉÂõæÂÉèÁöÑÂÖàÈ™åÁü•ËØÜ,ÂÖ∂‰∏ç‰ªÖËÉΩÊèêÂçáËØÜÂà´ÊïàÊûú,ËøòËÉΩÁõ∏ËæÉ‰∫é‰º†ÁªüÁöÑÂ∫¶ÈáèÂ≠¶‰π†Â§ßÂ§ßÁº©Áü≠ËÆ≠ÁªÉÊó∂Èó¥„ÄÇ3.Á¨¨ÂõõÁ´†Á≥ªÁªüÊÄßÁöÑÊÄªÁªì‰∫Ü‰∫∫ËÑ∏ÂõæÁâá‰∏≠ÂêÑÂõ†Á¥†‰πãÈó¥ÁöÑÂÖ≥Á≥ª,Âπ∂ÂÆö‰πâÂà∞Êõ¥ÂπøÊ≥õÁöÑÂ§öÊ†áÁ≠æÊï∞ÊçÆ‰∏ä„ÄÇÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÁâπÂæÅÁ∫ßÂØπÊäóËÆ≠ÁªÉÁöÑËß£ÊûÑÁΩëÁªú,Â∞ÜËæìÂÖ•ÂõæÁâáÂàÜËß£‰∏∫ÂØπ‰∏ªËØÜÂà´‰ªªÂä°(‰æãÂ¶ÇË∫´‰ªΩËØÜÂà´)ÂÖ∑ÊúâËæ®ËØÜÂäõÁöÑÁâπÂæÅ,ÊúüÊúõÂØπÂÖ∂È≤ÅÊ£íÁöÑÊúâÊ†áÁ≠æËØ≠‰πâÂ±ûÊÄß(Â¶ÇÂèØË°®ÊÉÖ„ÄÅÂÖâÁÖß„ÄÅÂåñÂ¶ÜÁ≠âÂ±ûÊÄß),‰ª•ÂèäÊúüÊúõÂØπÂÖ∂È≤ÅÊ£íÁöÑÊó†Ê†áÁ≠æÊàñÈöæ‰ª•ÈáèÂåñÁöÑÂõ†Á¥†(Â¶ÇËÉåÊôØÁ≠â)„ÄÇ‰∏âËÄÖ‰∫íË°•ËÄåÂèà‰∫íÁõ∏ËæπÈôÖÁã¨Á´ã„ÄÇ4.Á¨¨‰∫îÁ´†ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ∑±Â∫¶Â¢ûÂº∫Â≠¶‰π†ÁöÑÁÆóÊ≥ïÁî®‰∫éÊé¢Á¥¢ÈõÜ‰∏≠ÂêÑÂõæÁâáÁöÑÈáçË¶ÅÊÄß‰ª•Âèä‰∫íË°•ÊÄß„ÄÇÈÄöËøáÂú®IJB-A/B/CÁ≥ªÂàóÂü∫‰∫éÈõÜÁöÑ‰∫∫ËÑ∏ËØÜÂà´Êï∞ÊçÆÈõÜ,Âü∫‰∫éËßÜÈ¢ëÁöÑCelebrity-1000Êï∞ÊçÆÈõÜ,‰ª•ÂèäË°å‰∫∫ÈáçËØïÂà´‰ªªÂä°‰∏äÁöÑÂÆûÈ™åËØÅÂÆû‰∫ÜËØ•ÁÆóÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ  </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>





         <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Feature-level Frankenstein: Eliminating Variations for Discriminative Recognition.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/frankstein-cvpr.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://liu-xiaofeng.github.io/publications/A joint optimization.pdf">Feature-level Frankenstein: Eliminating Variations for Discriminative Recognition</a></b>
      <br> Xiaofeng Liu, Site Li, Lingsheng Kong, Wanqing Xie, Ping Jia, Jane You, B. V. K. Vijaya Kumar <br> <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2019.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Feature-level Frankenstein: Eliminating Variations for Discriminative Recognition.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Feature-level Frankenstein: Eliminating Variations for Discriminative Recognition.txt" target="_blank"> [BibTex] </a>
       <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Recent successes of deep learning-based recognition rely on maintaining the content related to the main-task label. However, how to explicitly dispel the noisy signals for better generalization remains an open issue. We systematically summarize the detrimental factors as task-relevant/irrelevant semantic variations and unspecified latent variation. In this paper, we cast these problems as an adversarial minimax game in the latent space. Specifically, we propose equipping an end-to-end conditional adversarial network with the ability to decompose an input sample into three complementary parts. The discriminative representation inherits the desired invariance property guided by prior knowledge of the task, which is marginally independent to the task-relevant/irrelevant semantic and latent variations. Our proposed framework achieves top performance on a serial of tasks, including digits recognition, lighting, makeup, disguise-tolerant face recognition, and facial attributes recognition.  </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>




         <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Hard negative generation for identity-disentangled facial expression recognition.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Hard negative generation for identity-disentangled facial expression recognition.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.sciencedirect.com/science/article/abs/pii/S0031320318303819">Hard negative generation for identity-disentangled facial expression recognition</a></b>
      <br> Xiaofeng Liu, BVK Vijaya Kumar, Ping Jia, Jane You <br> <i>Pattern Recognition </i>, 2019.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Hard negative generation for identity-disentangled facial expression recognition.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Hard negative generation for identity-disentangled facial expression recognition.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">  Various factors such as identity-specific attributes, pose, illumination and expression affect the appearance of face images. Disentangling the identity-specific factors is potentially beneficial for facial expression recognition (FER). Existing image-based FER systems either use hand-crafted or learned features to represent a single face image. In this paper, we propose a novel FER framework, named identity-disentangled facial expression recognition machine (IDFERM), in which we untangle the identity from a query sample by exploiting its difference from its references (e.g., its mined or generated frontal and neutral normalized faces). We demonstrate a possible ‚Äòrecognition via generation‚Äô scheme which consists of a novel hard negative generation (HNG) network and a generalized radial metric learning (RML) network. For FER, generated normalized faces are used as hard negative samples for metric learning. The difficulty of threshold validation and anchor selection are alleviated in RML and its distance comparisons are fewer than those of traditional deep metric learning methods. The expression representations of RML achieve superior performance on the CK‚ÄØ+‚ÄØ, MMI and Oulu-CASIA datasets, given a single query image for testing. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>




         <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/COL201917012301.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/COL201917012301.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.osapublishing.org/col/abstract.cfm?uri=col-17-1-012301">Electrically tunable holographic waveguide display based on holographic polymer dispersed liquid crystal grating</a></b>
      <br> Zhihui Diao, Lingsheng Kong, Junliang Yan, Junda Guo, Xiaofeng Liu, Li Xuan, Lei Yu <br> <i> Optics Letters </i>, 2019.
      <br> <a href="https://liu-xiaofeng.github.io/publications/COL201917012301.pdf"> [PDF] </a>
     <a href="https://liu-xiaofeng.github.io/bibtex/COL201917012301.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">  In this Letter, we present an electrically tunable holographic waveguide display (HWD) based on two slanted holographic polymer dispersed liquid crystal (HPDLC) gratings. Experimental results show that a see-through effect is obtained in the HWD that both the display light from HWD and the ambient light can be clearly seen simultaneously. By applying an external electric field, the output intensity of the display light can be modulated, which is attributed to the field-induced rotation of the liquid crystal molecules in the two HPDLC gratings. We also show that this electrically tunable performance enables the HWD to adapt to different ambient light conditions. This study provides some ideas towards the development of HWD and its application in augmented reality.

 </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>




         <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Attention Control with Metric Learning Alignment for Image Set-based Recognition.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Attention Control with Metric Learning Alignment for Image Set-based Recognition.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://arxiv.org/abs/1908.01872">Attention Control with Metric Learning Alignment for Image Set-based Recognition</a></b>
      <br> Xiaofeng Liu, Zhenhua Guo, Jane You, B.V.K Kumar<br> <i> arXiv:1908.01872</i>, 2018.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Attention Control with Metric Learning Alignment for Image Set-based Recognition.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Attention Control with Metric Learning Alignment for Image Set-based Recognition.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> This paper considers the problem of image set-based face verification and identification. Unlike traditional single sample (an image or a video) setting, this situation assumes the availability of a set of heterogeneous collection of orderless images and videos. The samples can be taken at different check points, different identity documents etc. The importance of each image is usually considered either equal or based on a quality assessment of that image independent of other images and/or videos in that image set. How to model the relationship of orderless images within a set remains a challenge. We address this problem by formulating it as a Markov Decision Process (MDP) in a latent space. Specifically, we first propose a dependency-aware attention control (DAC) network, which uses actor-critic reinforcement learning for attention decision of each image to exploit the correlations among the unordered images. An off-policy experience replay is introduced to speed up the learning process. Moreover, the DAC is combined with a temporal model for videos using divide and conquer strategies. We also introduce a pose-guided representation (PGR) scheme that can further boost the performance at extreme poses. We propose a parameter-free PGR without the need for training as well as a novel metric learning-based PGR for pose alignment without the need for pose detection in testing stage. Extensive evaluations on IJB-A/B/C, YTF, Celebrity-1000 datasets demonstrate that our method outperforms many state-of-art approaches on the set-based as well as video-based face recognition databases.  </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>



   <hr>
    <h2>2018</h2>

         <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Dependency-Aware Attention Control for Unconstrained Face Recognition with Image Sets.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Dependency-Aware Attention Control for Unconstrained Face Recognition with Image Sets.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="http://openaccess.thecvf.com/content_ECCV_2018/html/Xiaofeng_Liu_Dependency-aware_Attention_Control_ECCV_2018_paper.html">Dependency-aware Attention Control for Unconstrained Face Recognition with Image Sets</a></b>
      <br> Xiaofeng Liu, B.V.K. Vijaya Kumar, Chao Yang, Qingming Tang, Jane You <br> <i>European Conference on Computer Vision (ECCV)</i>, 2018.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Dependency-Aware Attention Control for Unconstrained Face Recognition with Image Sets.pdf"> [PDF] </a>
     <a href="https://liu-xiaofeng.github.io/bibtex/Dependency-Aware Attention Control for Unconstrained Face Recognition with Image Sets.txt" target="_blank"> [BibTex] </a>
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> This paper targets the problem of image set-based face verification and identification. Unlike traditional single media (an image or video) setting, we encounter a set of heterogeneous contents containing orderless images and videos. The importance of each image is usually considered either equal or based on their independent quality assessment. How to model the relationship of orderless images within a set remains a challenge. We address this problem by formulating it as a Markov Decision Process (MDP) at the feature level. Specifically, we propose a dependency-aware attention control (DAC) network, which resorts to actor-critic reinforcement learning for sequential attention decision of each image embedding to fully exploit the rich correlation cues among the unordered images. Moreover, its sample-efficient variant with off-policy experience replay is introduced to speed up the learning process. The pose-guided representation scheme can further boost the performance at the extremes of the pose variation. We show that our method leads to the state-of-the-art accuracy on IJB-A dataset and also generalizes well in several video-based face recognition tasks, extit {eg}, YTF and Celebrity-1000.  </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>







         <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Contextual-based Image Inpainting Infer, Match, and Translate.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/contex-eccv.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="http://openaccess.thecvf.com/content_ECCV_2018/html/Yuhang_Song_Contextual_Based_Image_ECCV_2018_paper.html">Contextual-based Image Inpainting: Infer, Match, and Translate</a></b>
      <br> Yuhang Song, Chao Yang, Zhe Lin, Xiaofeng Liu, Qin Huang, Hao Li, C-C Jay Kuo <br> <i>European Conference on Computer Vision (ECCV) </i>, 2018.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Contextual-based Image Inpainting Infer, Match, and Translate.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Contextual-based Image Inpainting Infer, Match, and Translate.txt" target="_blank"> [BibTex] </a>
       <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">  We study the task of image inpainting, which is to fill in the missing region of an incomplete image with plausible contents. To this end, we propose a learning-based approach to generate visually coherent completion given a high-resolution image with missing components. In order to overcome the difficulty to directly learn the distribution of high-dimensional image data, we divide the task into inference and translation as two separate steps and model each step with a deep neural network. We also use simple heuristics to guide the propagation of local textures from the boundary to the hole. We show that, by using such techniques, inpainting reduces to the problem of learning two image-feature translation functions in much smaller space and hence easier to train. We evaluate our method on several public datasets and show that we generate results of better visual quality than previous state-of-the-art methods. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>



         <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Ordinal Regression with Neuron Stick-Breaking for Medical Diagnosis.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/ordinal-eccvw.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="http://openaccess.thecvf.com/content_eccv_2018_workshops/w33/html/Liu_Ordinal_Regression_with_Neuron_Stick-breaking_for_Medical_Diagnosis_ECCVW_2018_paper.html">Ordinal Regression with Neuron Stick-Breaking for Medical Diagnosis</a></b>
      <br> Xiaofeng Liu, Yang Zou, Yuhang Song, Chao Yang, Jane You, BVK Vijaya Kumar <br> <i>European Conference on Computer Vision (ECCV) </i>, 2018 Workshops.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Ordinal Regression with Neuron Stick-Breaking for Medical Diagnosis.pdf"> [PDF] </a>
     <a href="https://liu-xiaofeng.github.io/bibtex/Ordinal Regression with Neuron Stick-Breaking for Medical Diagnosis.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> The classification for medical diagnosis usually involves inherently ordered labels corresponding to the level of health risk. Previous multi-task classifiers on ordinal data often use several binary classification branches to compute a series of cumulative probabilities. However, these cumulative probabilities are not guaranteed to be monotonically decreasing. It also introduces a large number of hyper-parameters to be fine-tuned manually. This paper aims to eliminate or at least largely reduce the effects of those problems. We propose a simple yet efficient way to rephrase the output layer of the conventional deep neural network. We show that our methods lead to the state-of-the-art accuracy on Diabetic Retinopathy dataset and Ultrasound Breast dataset with very little additional cost.  </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>


    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Data Augmentation via Latent Space Interpolation for Image Classification.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Data Augmentation via Latent Space Interpolation for Image Classification.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://ieeexplore.ieee.org/abstract/document/8545506">Data Augmentation via Latent Space Interpolation for Image Classification</a></b>
      <br> Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun Wang, Site Li, Ping Jia, Jane You <br> <i>International Conference on Pattern Recognition (ICPR) </i>, 2018.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Data Augmentation via Latent Space Interpolation for Image Classification.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Data Augmentation via Latent Space Interpolation for Image Classification.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;">  Effective training of the deep neural networks requires much data to avoid underdetermined and poor generalization. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data by for example, flipping, distorting, adding noise to, cropping a patch from the original samples. In this paper, we introduce the adversarial autoencoder (AAE) to impose the feature representations with uniform distribution and apply the linear interpolation on latent space, which is potential to generate a much broader set of augmentations for image classification. As a possible ‚Äúrecognition via generation‚Äù framework, it has potentials for several other classification tasks. Our experiments on the ILSVRC 2012, CIFAR-10 datasets show that the latent space interpolation (LSI) improves the generalization and performance of state-of-the-art deep neural networks. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>







    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/A joint optimization framework of low-dimensional projection and collaborative representation for discriminative classification.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/joint-icpr.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://ieeexplore.ieee.org/abstract/document/8545267">A joint optimization framework of low-dimensional projection and collaborative representation for discriminative classification</a></b>
      <br> Xiaofeng Liu, Zhaofeng Li, Lingsheng Kong, Zhihui Diao, Junliang Yan, Yang Zou, Chao Yang, Ping Jia, Jane You <br> <i>International Conference on Pattern Recognition (ICPR) </i>, 2018 1493-1498.
      <br> <a href="https://liu-xiaofeng.github.io/publications/A joint optimization framework of low-dimensional projection and collaborative representation for discriminative classification.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/A joint optimization framework of low-dimensional projection and collaborative representation for discriminative classification.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Various representation-based methods have been developed and shown great potential for pattern classification. To further improve their discriminability, we propose a Bi-level optimization framework in terms of both low-dimensional projection and collaborative representation. Specifically, during the projection phase, we try to minimize the intra-class similarity and inter-class dissimilarity, while in the representation phase, our goal is to achieve the lowest correlation of the representation results. Solving this joint optimization mutually reinforces both aspects of feature projection and representation. Experiments on face recognition, object categorization and scene classification dataset demonstrate remarkable performance improvements led by the proposed framework. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>



    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Image Inpainting using Block-wise Procedural Training with Annealed Adversarial Counterpar.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/block-wise.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://arxiv.org/abs/1803.08943">Image Inpainting using Block-wise Procedural Training with Annealed Adversarial Counterpar</a></b>
      <br> Chao Yang, Yuhang Song, Xiaofeng Liu, Qingming Tang, C-C Jay Kuo <br> <i>arXiv: 1803.08943</i>.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Image Inpainting using Block-wise Procedural Training with Annealed Adversarial Counterpar.pdf"> [PDF] </a>
     <a href="https://liu-xiaofeng.github.io/bibtex/Image Inpainting using Block-wise Procedural Training with Annealed Adversarial Counterpar.txt" target="_blank"> [BibTex] </a>
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Recent advances in deep generative models have shown promising potential in image inpanting, which refers to the task of predicting missing pixel values of an incomplete image using the known context. However, existing methods can be slow or generate unsatisfying results with easily detectable flaws. In addition, there is often perceivable discontinuity near the holes and require further post-processing to blend the results. We present a new approach to address the difficulty of training a very deep generative model to synthesize high-quality photo-realistic inpainting. Our model uses conditional generative adversarial networks (conditional GANs) as the backbone, and we introduce a novel block-wise procedural training scheme to stabilize the training while we increase the network depth. We also propose a new strategy called adversarial loss annealing to reduce the artifacts. We further describe several losses specifically designed for inpainting and show their effectiveness. Extensive experiments and user-study show that our approach outperforms existing methods in several tasks such as inpainting, face completion and image harmonization. Finally, we show our framework can be easily used as a tool for interactive guided inpainting, demonstrating its practical value to solve common real-world challenges. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>







     <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Adaptive metric learning with deep neural networks for video-based facial expression recognition.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Adaptive metric learning with deep neural networks for video-based facial expression recognition.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.spiedigitallibrary.org/journals/Journal-of-Electronic-Imaging/volume-27/issue-1/013022/Adaptive-metric-learning-with-deep-neural-networks-for-video-based/10.1117/1.JEI.27.1.013022.short?SSO=1">Adaptive metric learning with deep neural networks for video-based facial expression recognition</a></b>
      <br> Xiaofeng Liu, Yubin Ge, Chao Yang, Ping Jia <br> <i>Journal of Electronic Imaging</i>, 013022.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Adaptive metric learning with deep neural networks for video-based facial expression recognition.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Adaptive metric learning with deep neural networks for video-based facial expression recognition.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Video-based facial expression recognition has become increasingly important for plenty of applications in the real world. Despite that numerous efforts have been made for the single sequence, how to balance the complex distribution of intra- and interclass variations well between sequences has remained a great difficulty in this area. We propose the adaptive ( N+M)-tuplet clusters loss function and optimize it with the softmax loss simultaneously in the training phrase. The variations introduced by personal attributes are alleviated using the similarity measurements of multiple samples in the feature space with many fewer comparison times as conventional deep metric learning approaches, which enables the metric calculations for large data applications (e.g., videos). Both the spatial and temporal relations are well explored by a unified framework that consists of an Inception-ResNet network with long short term memory and the two fully connected layer branches structure. Our proposed method has been evaluated with three well-known databases, and the experimental results show that our method outperforms many state-of-the-art approaches. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>



    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Panoramic stereo imaging system for efficient.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Panoramic stereo imaging system for efficient.jpg" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.osapublishing.org/ao/abstract.cfm?uri=ao-57-3-396">Panoramic stereo imaging system for efficient mosaicking: parallax analyses and system design</a></b>
      <br> Junliang Yan, Lingsheng Kong, Zhihui Diao, Xiaofeng Liu, Lilu Zhu, Ping Jia <br> <i>Applied optics</i>, 396-403.
      <br> <a href="https://liu-xiaofeng.github.io/publications/Panoramic stereo imaging system for efficient.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Panoramic stereo imaging system for efficient.txt" target="_blank"> [BibTex] </a>
       <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Panoramic stereo images, captured by distributed devices then mosaicking, are competent contents for virtual reality applications. Mosaicking raw images with different perspectives into satisfying final results is still not efficient enough, even if state-of-the-art algorithms are employed. For improving this efficiency in optical methods, we delve into the potential of the capturing system. Two parallax factors, peak parallax and deviation of parallaxes, are proposed to assess the mosaicking capability. By controlling variables and numerical computation, rules between parallax factors and design parameters have been revealed. Validation by simulations, large capturing distance, more cameras, compact arrangement, and moderate overlaps are suggested as the general design strategy. Benefiting from efficient mosaicking, systems based on our design strategy would have potential for real-time applications. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>


    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Normalized face image generation with perceptron generative adversarial networks.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Normalized face image generation with perceptron generative adversarial networks.png" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://ieeexplore.ieee.org/abstract/document/8311462">Normalized face image generation with perceptron generative adversarial networks</a></b>
        <br> Xiaofeng Liu, BVK Vijaya Kumar, Yubin Ge, Chao Yang, Jane You, Ping Jia <br> <i>IEEE International Conference on Identity, Security, and Behavior Analysis (ISBA)</i>, 2018. <b><a href="https://liu-xiaofeng.github.io/publications/figs/award.jpg"> [IBM Best Paper Award]</a>a></b>
      <br> <a href="https://liu-xiaofeng.github.io/publications/Normalized face image generation with perceptron generative adversarial networks.pdf"> [PDF] </a>
     <a href="https://liu-xiaofeng.github.io/bibtex/Normalized face image generation with perceptron generative adversarial networks.txt" target="_blank"> [BibTex] </a>
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> This paper presents a deep neural architecture for synthesizing the frontal and neutral facial expression image of a subject given a query face image with arbitrary expression. This is achieved by introducing a combination of feature space perceptual loss, pixel-level loss, adversarial loss, symmetry loss, and identity-preserving loss. We leverage both the frontal and neutral face distributions and pre-trained discriminative deep perceptron models to guide the identity-preserving inference of the normalized views from expressive profiles. Unlike previous generative methods that utilize their intermediate features for the recognition tasks, the resulting expression- and pose-disentangled face image has potential for several downstream applications, such as facial expression or face recognition, and attribute estimation. We show that our approach produces photorealistic and coherent results, which assist the deep metric learning-based facial expression recognition (FER) to achieve promising results on two well-known FER datasets. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>


    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/CN108492751A.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/CN108492751A.bmp" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://patents.google.com/patent/CN108492751A/zh">‰∏ÄÁßçÂõæÂÉèÊÇ¨ÊµÆÊòæÁ§∫Ë£ÖÁΩÆÂèä3DÊòæÁ§∫Âô®ÔºàImage suspension display device and 3D displayÔºâ</a></b>
      <br> CN106021803A <br> <i>CN Patent</i>.
      <br> <a href="https://liu-xiaofeng.github.io/publications/CN108492751A.pdf"> [PDF] </a>

        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Êú¨ÂèëÊòéÊèê‰æõÁöÑÂõæÂÉèÊÇ¨ÊµÆÊòæÁ§∫Ë£ÖÁΩÆÔºåÂà©Áî®‰∫ÜLEDÊòæÁ§∫ÈòµÂàóÂçïÂÖÉÁöÑÊòæÁ§∫Âå∫Âüü‰∏éÂèçÂ∞ÑÁéªÁíÉÁöÑÂèçÂ∞ÑÂå∫ÂüüÁõ∏ÂåπÈÖçÔºåÁ™ÅÁ†¥‰∫ÜÊòæÁ§∫ÂÖâÊ∫êÂàÜËæ®ÁéáÂØπÊÇ¨ÊµÆÂõæÂÉèÁöÑÁîªÂπÖÂ§ßÂ∞èÁöÑÈôêÂà∂ÔºåËß£ÂÜ≥‰∫ÜÊòæÁ§∫ÂÖâÊ∫êÁöÑÂàÜËæ®ÁéáÂà©Áî®Áéá‰∏çÈ´òÁöÑÈóÆÈ¢òÔºåÂêåÊó∂È¶ñÊ¨°ÈááÁî®ÂõûÂ≠óÂûãLEDÊòæÁ§∫ÈòµÂàóÔºåÂ§ßÂ§ßÊèêÈ´ò‰∫ÜÂõõ‰∏™ËßíÁöÑÂ±èÂπïÂà©Áî®Áéá„ÄÇÂè¶Â§ñÔºåËØ•Á≥ªÁªüËøòÂ¢ûÂä†‰∫ÜÊ∂àÊùÇÂÖâÁöÑÊª§ÂÖâÈïúÁ≥ªÁªüÔºåÊèêÈ´ò‰∫ÜÂä®ÊÄÅÂõæÂÉèÊÇ¨ÊµÆÁöÑÂØπÊØîÂ∫¶ÔºåÂ¢ûÂº∫‰∫ÜÊÇ¨ÊµÆËßÜËßâÁöÑÊïàÊûúÔºåÊú¨ÂèëÊòé‰ΩøÂä®ÊÄÅÂõæÂÉèÊÇ¨ÊµÆÁöÑÁîªÂπÖÂ∞∫ÂØ∏Âæó‰ª•Â¢ûÂ§ßÔºåÊï¥‰∏™Á≥ªÁªüÁöÑËßÇÂØüÊñπÂêëÁöÑÊ®™ÂêëÁõ∏ÂØπÂ∞∫ÂØ∏Âæó‰ª•ÂáèÂ∞èÔºå‰∏∫Â§ßÂ∞∫ÂØ∏ÊÇ¨ÊµÆÊòæÁ§∫Â±èÁöÑËÆæËÆ°Êèê‰æõ‰∫ÜÊñπ‰æøÔºåÂπ∂‰∏î‰ΩøËßÇÂØüËÄÖÂèØ‰ª•Âú®Êõ¥Âä†ËàíÈÄÇÁöÑËßÜËßâËåÉÂõ¥ÂÜÖÂØπÊâÄÊòæÁ§∫ÁöÑÂä®ÊÄÅÂõæÂÉèËøõË°åËßÇÂØü„ÄÇÊú¨ÂèëÊòéËøòÊèê‰æõ‰∏ÄÁßç3DÊòæÁ§∫Âô®„ÄÇ </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>


    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/CN109106567A.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/CN109106567A.bmp" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://patents.google.com/patent/CN109106567A/zh">‰∏ÄÁßçÁî®‰∫éËøëËßÜÊ≤ªÁñóÁöÑÂ§öËßÜË∑ùÊòæÁ§∫Á≥ªÁªüÔºàMulti-line-of-sight display system for myopia treatmentÔºâ</a></b>
      <br> CN106021803A <br> <i>CN Patent</i>.
      <br> <a href="https://liu-xiaofeng.github.io/publications/CN109106567A.pdf"> [PDF] </a>

        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Êú¨ÂèëÊòéÊèê‰æõ‰∫Ü‰∏ÄÁßçÁî®‰∫éËøëËßÜÊ≤ªÁñóÁöÑÂ§öËßÜË∑ùÊòæÁ§∫Á≥ªÁªüÔºåÂåÖÊã¨ÔºöÂ§¥Êà¥ÂºèË£ÖÁΩÆÔºå‰ª•ÂèäÂõ∫ÂÆöÂú®ÊâÄËø∞Â§¥Êà¥ÂºèË£ÖÁΩÆÂÜÖÁöÑÂÆûÊó∂Â§ÑÁêÜÊ®°Âùó„ÄÅÂèåÁõÆÊòæÁ§∫Ê®°ÂùóÂíåÂèØÂèòÁÑ¶ÂÖâÂ≠¶Ê®°ÂùóÔºõÊâÄËø∞ÂÆûÊó∂Â§ÑÁêÜÊ®°ÂùóÁî®‰∫é‰æùÊçÆ‰ΩøÁî®ËÄÖËß¶ÂèëÁöÑËßÜÈ¢ëÂõæÂÉèÊéßÂà∂Êåá‰ª§ÁîüÊàêËßÜÈ¢ëÂõæÂÉèÈ©±Âä®‰ø°Âè∑ÔºõÊâÄËø∞ÂèåÁõÆÊòæÁ§∫Ê®°ÂùóÁî®‰∫é‰æùÊçÆÊâÄËø∞ËßÜÈ¢ëÂõæÂÉèÈ©±Âä®‰ø°Âè∑ÔºåÊòæÁ§∫Áõ∏ÂØπÂ∫îÁöÑËßÜÈ¢ëÂõæÂÉèÔºõÊâÄËø∞ÂèØÂèòÁÑ¶ÂÖâÂ≠¶Ê®°ÂùóÁî®‰∫éÊîπÂèòÊâÄËø∞ËßÜÈ¢ëÂõæÂÉèÊàêÂÉè‰ΩçÁΩÆ‰∏é‰ΩøÁî®ËÄÖÁúºÁùõ‰πãÈó¥ÁöÑË∑ùÁ¶ª„ÄÇËØ•Â§öËßÜË∑ùÊòæÁ§∫Á≥ªÁªüÂèØ‰ª•Êõ¥‰∏∫ÊúâÊïàÁöÑËøõË°åËøëËßÜÊ≤ªÁñó„ÄÇ </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>



    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/CN108492724A.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/CN108492724A.bmp" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://patents.google.com/patent/CN108492724A/zh">‰∏ÄÁßçÈÄÇÂ∫îËßÜËßâÁâπÊÄßÁöÑÊ≤âÊµ∏ÂºèÊòæÁ§∫Ë£ÖÁΩÆÂèäVRËÆæÂ§áÔºàImmersive display device and VR equipment adapted to visual characteristicsÔºâ</a></b>
      <br> CN106021803A <br> <i>CN Patent</i>.
      <br> <a href="https://liu-xiaofeng.github.io/publications/CN108492724A"> [PDF] </a>

        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Êú¨ÂèëÊòéÊèê‰æõÁöÑÈÄÇÂ∫îËßÜËßâÁâπÊÄßÁöÑÊ≤âÊµ∏ÂºèÊòæÁ§∫Ë£ÖÁΩÆÂèäVRËÆæÂ§áÔºåÁî±È´òÂàÜËæ®Áéá„ÄÅÂ∞èÈó¥Ë∑ùÁöÑ‰∏≠ÂøÉÂ±èÂíåÂ§öÂùóËæÉ‰ΩéÂàÜËæ®ÁéáÁöÑÂÖ∂‰ªñÂ±èÊûÑÊàêÔºåÂèØ‰ª•Êª°Ë∂≥ËßÇÁúãËÄÖËßÜËßâ‰∏≠ÂøÉÂèäËæπÁºò‰∏çÂêåÂàÜËæ®ÁéáË¶ÅÊ±ÇÔºå‰∏îÂ§ö‰∫∫ÂêåÊó∂ËßÇÁúãÂ§±ÁúüÂ∞è„ÄÇÊòæÁ§∫Ë£ÖÁΩÆÂèäËÆæÂ§áÈááÁî®ÂÖ∑ÊúâËá™ÂèëÂÖâÁâπÊÄßÁöÑCOBÂ∞èÈó¥Ë∑ùLEDÊòæÁ§∫Âô®‰ª∂ÔºåÂÖ∂‰∫ÆÂ∫¶„ÄÅËâ≤ÂΩ©È•±ÂíåÂ∫¶È´òÔºåËßÇÁúãËøáÁ®ãÂèóÁéØÂ¢ÉÁÖßÊòéÊù°‰ª∂ÂΩ±ÂìçÂ∞èÔºåÂπ∂ÂèØÂÆûÁé∞Êó†ÁºùÊãºÊé•ÔºåÂõæÂÉè‰∏ÄËá¥ÊÄßÈ´ò„ÄÇÂú®‰∏≠ÂøÉÂ±èËææÂà∞‰∫∫ÁúºÊûÅÈôêÂàÜËæ®ÁéáÁöÑÂâçÊèê‰∏ãÔºåËßÜËßâËæπÁºòÂØπÂ∫îÁöÑÂÖ∂‰ªñÂ±èÈááÁî®ËæÉ‰ΩéÂàÜËæ®ÁéáÁöÑLEDÂ±èÔºåÂ§ßÂπÖÈôç‰Ωé‰∫ÜÂÆûÁî®ÊàêÊú¨Âπ∂‰∏îÁ¨¶Âêà‰∫∫ÁúºËßÜËßâÁâπÊÄß„ÄÇÊâÄÊúâÊòæÁ§∫Â±èÂùáÈááÁî®Ê®°ÂùóÂåñÁÆ±‰ΩìËÆæËÆ°ÔºåÊñπ‰æøÂÆâË£Ö„ÄÅÊãÜÂç∏ÂèäËøêËæì„ÄÇ </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>



   <hr>
    <h2>2017</h2>

    <table border="0">
    <tbody><tr>
    <td width="140"><a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w6/papers/Liu_Adaptive_Deep_Metric_CVPR_2017_paper.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Adaptive Deep Metric Learning for Identity-Aware Facial Expression Recognition.bmp" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://ieeexplore.ieee.org/abstract/document/8014813">Adaptive Deep Metric Learning for Identity-Aware Facial Expression Recognition</a></b>
      <br> Xiaofeng Liu, B.V.K Vijaya Kumar, Jane You, Ping Jia <br> <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</i>, 2017.<b>[Oral]</b>
      <br> <a href="https://liu-xiaofeng.github.io/publications/Adaptive Deep Metric Learning for Identity-Aware Facial Expression Recognition.pdf"> [PDF] </a>
     <a href="https://liu-xiaofeng.github.io/bibtex/Adaptive Deep Metric Learning for Identity-Aware Facial Expression Recognition.txt" target="_blank"> [BibTex] </a>
       <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> A key challenge of facial expression recognition (FER) is to develop effective representations to balance the complex distribution of intra- and inter- class variations. The latest deep convolutional networks proposed for FER are trained by penalizing the misclassification of images via the softmax loss. In this paper, we show that better FER performance can be achieved by combining the deep metric loss and softmax loss in a unified two fully connected layer branches framework via joint optimization. A generalized adaptive (N+M)-tuplet clusters loss function together with the identity-aware hard-negative mining and online positive mining scheme are proposed for identity-invariant FER. It reduces the computational burden of deep metric learning, and alleviates the difficulty of threshold validation and anchor selection. Extensive evaluations demonstrate that our method outperforms many state-of-art approaches on the posed as well as spontaneous facial expression databases. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>


    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/Line-scan system for continuous hand authentication.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/Line-scan system for continuous hand authentication.bmp" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://www.spiedigitallibrary.org/journals/Optical-Engineering/volume-56/issue-3/033106/Line-scan-system-for-continuous-hand-authentication/10.1117/1.OE.56.3.033106.short?SSO=1">Line-scan system for continuous hand authentication</a></b>
      <br> Xiaofeng Liu, Lingsheng Kong, Zhihui Diao, Ping Jia <br> <i>Optical Engineering</i>, 56(3), 033106 (2017).
      <br> <a href="https://liu-xiaofeng.github.io/publications/Line-scan system for continuous hand authentication.pdf"> [PDF] </a>
      <a href="https://liu-xiaofeng.github.io/bibtex/Line-scan system for continuous hand authentication.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> An increasing number of heavy machinery and vehicles have come into service, giving rise to a significant concern over protecting these high-security systems from misuse. Conventionally, authentication performed merely at the initial login may not be sufficient for detecting intruders throughout the operating session. To address this critical security flaw, a line-scan continuous hand authentication system with the appearance of an operating rod is proposed. Given that the operating rod is occupied throughout the operating period, it can be a possible solution for unobtrusively recording the personal characteristics for continuous monitoring. The ergonomics in the physiological and psychological aspects are fully considered. Under the shape constraints, a highly integrated line-scan sensor, a controller unit, and a gear motor with encoder are utilized. This system is suitable for both the desktop and embedded platforms with a universal serial bus interface. The volume of the proposed system is smaller than 15% of current multispectral area-based camera systems. Based on experiments on a database with 4000 images from 200 volunteers, a competitive equal error rate of 0.1179% is achieved, which is far more accurate than the state-of-the-art continuous authentication systems using other modalities. </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>















  <hr>
    <h2>2016</h2>

    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/CN106021803B.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/CN106021803B.bmp" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://patents.google.com/patent/CN106021803B/zh">‰∏ÄÁßçÁ°ÆÂÆöÂõæÂÉèÈááÈõÜËÆæÂ§áÊúÄ‰ºòÊéíÂ∏ÉÁöÑÊñπÊ≥ïÂèäÁ≥ªÁªüÔºàMethod and system for determining optimal arrangement of image acquisition equipmentÔºâ</a></b>
      <br> CN106021803A <br> <i>CN Patent</i>.
      <br> <a href="https://liu-xiaofeng.github.io/publications/CN106021803B.pdf"> [PDF] </a>
     <a href="https://liu-xiaofeng.github.io/bibtex/‰∏ÄÁßçÁ°ÆÂÆöÂõæÂÉèÈááÈõÜËÆæÂ§áÊúÄ‰ºòÊéíÂ∏ÉÁöÑÊñπÊ≥ïÂèäÁ≥ªÁªü.txt" target="_blank"> [BibTex] </a>
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Êú¨Áî≥ËØ∑Êèê‰æõ‰∫Ü‰∏ÄÁßçÁ°ÆÂÆöÂõæÂÉèÈááÈõÜËÆæÂ§áÊúÄ‰ºòÊéíÂ∏ÉÁöÑÊñπÊ≥ïÂèäÁ≥ªÁªüÔºåÊûÑÂª∫ÁõÆÊ†áÂú∫Âú∞ÁöÑ‰∏âÁª¥Ê®°ÂûãÔºõÂú®ÁõÆÊ†áÂú∫Âú∞ÁöÑ‰∏âÁª¥Ê®°Âûã‰∏≠Âä†ÂÖ•È¢ÑÂÖàËé∑ÂèñÁöÑÁõÆÊ†áÊ†áËÆ∞ÁÇπÁöÑ‰∏âÁª¥‰ΩçÁΩÆÊï∞ÊçÆÔºõÂú®ÁõÆÊ†áÂú∫Âú∞ÁöÑ‰∏âÁª¥Ê®°Âûã‰∏≠ÔºåÈÄöËøá‰∏âÁª¥‰ΩçÁΩÆÊï∞ÊçÆÁ°ÆÂÆöÁõÆÊ†áÂ§πËßí‰ª•Âèä‰∏éÂêÑ‰∏™ÂõæÂÉèÈááÈõÜËÆæÂ§áÂØπÂ∫îÁöÑÁõÆÊ†áË∑ùÁ¶ª„ÄÅÁõÆÊ†áËßÜÂú∫ËßíÔºåÁõÆÊ†áÂ§πËßí‰∏∫Â§ö‰∏™ÂõæÂÉèÈááÈõÜËÆæÂ§áËøõË°å‰∏§‰∏§ÁªÑÂêàËé∑ÂæóÁöÑÂêÑ‰∏™ÂõæÂÉèÈááÈõÜËÆæÂ§áÂØπ‰∏≠‰∏§‰∏™ÂõæÂÉèÈááÈõÜËÆæÂ§áÁöÑÂ§πËßíÔºõÂü∫‰∫éÈ¢ÑÂÖàËé∑ÂæóÁöÑÂêÑ‰∏™ÂõæÂÉèÈááÈõÜËÆæÂ§áÁöÑÂèØËßÜË∑ùÁ¶ªËåÉÂõ¥„ÄÅËßÜÂú∫ËßíËåÉÂõ¥ÔºåÂà©Áî®ÁõÆÊ†áË∑ùÁ¶ª„ÄÅÁõÆÊ†áËßÜÂú∫ËßíÂíåÁõÆÊ†áÂ§πËßíÔºåÊåâÈ¢ÑËÆæÁöÑËÆ°ÁÆóËßÑÂàôÁ°ÆÂÆöÂ§ö‰∏™ÂõæÂÉèÈááÈõÜËÆæÂ§áÂú®ÁõÆÊ†áÂú∫Âú∞‰∏≠ÁöÑÊúÄ‰ºòÊéíÂ∏É„ÄÇÊú¨Áî≥ËØ∑ÂèØËé∑ÂæóÂõæÂÉèÈááÈõÜËÆæÂ§áÂú®ÁõÆÊ†áÂú∫Âú∞‰∏≠ÁöÑÊúÄ‰ºòÊéíÂ∏ÉÊñπÊ°àÔºåÂü∫‰∫éËØ•ÊúÄ‰ºòÊéíÂ∏ÉÊñπÊ≥ïÂèØËé∑ÂæóËæÉÂ•ΩÁöÑËøêÂä®ÊçïÊçâÊïàÊûú„ÄÇ </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>


    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/CN106056089B.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/CN106056089B.bmp" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://patents.google.com/patent/CN106056089B/zh">‰∏ÄÁßç‰∏âÁª¥ÂßøÊÄÅËØÜÂà´ÊñπÊ≥ïÂèäÁ≥ªÁªüÔºàA 3D pose recognition methods and systemÔºâ</a></b>
      <br> CN106056089A <br> <i>CN Patent</i>.
      <br> <a href="https://liu-xiaofeng.github.io/publications/CN106056089B.pdf"> [PDF] </a>
     <a href="https://liu-xiaofeng.github.io/bibtex/‰∏ÄÁßç‰∏âÁª¥ÂßøÊÄÅËØÜÂà´ÊñπÊ≥ïÂèäÁ≥ªÁªü.txt" target="_blank"> [BibTex] </a>
      <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Êú¨Áî≥ËØ∑Êèê‰æõ‰∫Ü‰∏ÄÁßç‰∏âÁª¥ÂßøÊÄÅËØÜÂà´ÊñπÊ≥ïÂèäÁ≥ªÁªüÔºåËé∑ÂèñÂ§ö‰∏™ËßÜËßíÁöÑÁõÆÊ†áÂõæÂÉèÂ∏ßÂ∫èÂàóÔºåÁõÆÊ†áÂõæÂÉèÂ∏ßÂ∫èÂàó‰∏≠ÂåÖÊã¨ÂæÖËØÜÂà´ÁöÑÁõÆÊ†áÂõæÂÉèÂ∏ßÔºõÂàÜÂà´ÂØπÂêÑ‰∏™ËßÜËßíÁöÑÁõÆÊ†áÂõæÂÉèÂ∏ßÂ∫èÂàóËøõË°åËΩÆÂªìÊèêÂèñÔºåËé∑ÂæóÂêÑ‰∏™ËßÜËßíÁöÑÁõÆÊ†áËΩÆÂªìÂõæÂÉèÂ∏ßÂ∫èÂàóÔºõ‰ªéÈ¢ÑÂ≠òÁöÑÂ§ö‰∏™ÂèÇËÄÉËΩÆÂªìÂõæÂÉèÂ∏ßÂ∫èÂàó‰∏≠ÔºåÂàÜÂà´Á°ÆÂÆöÂá∫‰∏éÂêÑ‰∏™ËßÜËßíÁöÑÁõÆÊ†áËΩÆÂªìÂõæÂÉèÂ∏ßÂ∫èÂàóÂåπÈÖçÁöÑÂèÇËÄÉËΩÆÂªìÂõæÂÉèÂ∏ßÂ∫èÂàó‰Ωú‰∏∫ÂêÑ‰∏™ËßÜËßíÁöÑÁõÆÊ†áÂèÇËÄÉËΩÆÂªìÂõæÂÉèÂ∏ßÂ∫èÂàóÔºõÂü∫‰∫éÈ¢ÑÂ≠òÁöÑ‰∏éÂêÑ‰∏™ÂèÇËÄÉËΩÆÂªìÂõæÂÉèÂ∏ßÂ∫èÂàó‰∏≠ÂêÑ‰∏™ÂõæÂÉèÂ∏ßÂØπÂ∫îÁöÑÂßøÊÄÅ‰ø°ÊÅØÔºå‰ªéÂêÑ‰∏™ËßÜËßíÁöÑÁõÆÊ†áÂèÇËÄÉËΩÆÂªìÂõæÂÉèÂ∏ßÂ∫èÂàó‰∏≠Á°ÆÂÆöÂá∫‰∏éÁõÆÊ†áÂõæÂÉèÂ∏ßÂØπÂ∫îÁöÑÂèÇËÄÉËΩÆÂªìÂõæÂÉèÂ∏ßÊâÄÂØπÂ∫îÁöÑÂßøÊÄÅ‰ø°ÊÅØÔºå‰Ωú‰∏∫ÂêÑ‰∏™ËßÜËßíÁöÑ‰∫åÁª¥ÂßøÊÄÅ‰ø°ÊÅØÔºõÈÄöËøá‰∫åÁª¥ÂßøÊÄÅ‰ø°ÊÅØÁ°ÆÂÆö‰∏âÁª¥ÂßøÊÄÅ‰ø°ÊÅØ„ÄÇÊú¨Áî≥ËØ∑ÂèØÂ∫îÁî®‰∫éÂÆ§Â§ñÂ§ßÂûãËøêÂä®Âú∫ÊôØÔºå‰∏îÊèêÂçá‰∫ÜÂßøÊÄÅËØÜÂà´ÁöÑËØÜÂà´Áéá„ÄÇ </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>


    <table border="0">
    <tbody><tr>
    <td width="140"><a href="https://liu-xiaofeng.github.io/publications/CN106022269A.pdf"><img src="https://liu-xiaofeng.github.io/publications/figs/CN106022269A.bmp" border="1"width="210"></a></td>
    <td width="20"></td>
    <td valign="middle" width="900">
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://patents.google.com/patent/CN106022269A/zh">‰∏ÄÁßç‰∫∫ËÑ∏ÈÖçÂáÜÊñπÊ≥ïÂèäË£ÖÁΩÆÔºàA face registration method and deviceÔºâ</a></b>
      <br> CN106022269A <br> <i>CN Patent</i>.
      <br> <a href="https://liu-xiaofeng.github.io/publications/CN106022269A.pdf"> [PDF] </a>
     <a href="https://liu-xiaofeng.github.io/bibtex/‰∏ÄÁßç‰∫∫ËÑ∏ÈÖçÂáÜÊñπÊ≥ïÂèäË£ÖÁΩÆ.txt" target="_blank"> [BibTex] </a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Êú¨Áî≥ËØ∑Êèê‰æõ‰∫Ü‰∏ÄÁßç‰∫∫ËÑ∏ÈÖçÂáÜÊñπÊ≥ïÂèäË£ÖÁΩÆÔºåÈ¢ÑÂÖàËÆæÂÆö‰ΩéÂàÜËæ®ÁéáÁΩëÊ†ºÊ®°ÊùøÂíåÈ´òÂàÜËæ®ÁéáÁΩëÊ†ºÊ®°ÊùøÔºåÂà©Áî®‰ΩéÂàÜËæ®ÁéáÁΩëÊ†ºÊ®°ÊùøÂØπÁõÆÊ†á‰∫∫ËÑ∏ÂõæÂÉèËøõË°åÈÖçÂáÜÔºåËé∑Âæó‰ΩéÂàÜËæ®ÁéáÈÖçÂáÜÁªìÊûúÔºõÈÄöËøá‰ΩéÂàÜËæ®ÁéáÈÖçÂáÜÁªìÊûúÂíåÁΩëÊ†ºËΩ¨Êç¢Áü©ÈòµÁ°ÆÂÆöÈ´òÂàÜËæ®ÁéáÈÖçÂáÜÂèÇËÄÉÂÄºÔºåÁΩëÊ†ºËΩ¨Êç¢Áü©ÈòµÁî®‰∫éÂÆûÁé∞‰ΩéÂàÜËæ®ÁéáÁΩëÊ†ºÊ®°Êùø‰∏éÈ´òÂàÜËæ®ÁéáÁΩëÊ†ºÊ®°Êùø‰πãÈó¥ÁöÑËΩ¨Êç¢ÔºõÂü∫‰∫éÈ´òÂàÜËæ®ÁéáÈÖçÂáÜÂèÇËÄÉÂÄºÔºåÂà©Áî®È´òÂàÜËæ®ÁéáÁΩëÊ†ºÊ®°ÊùøÂØπÁõÆÊ†á‰∫∫ËÑ∏ÂõæÂÉèËøõË°åÈÖçÂáÜÔºåËé∑ÂæóÊúÄÁªàÁöÑÈÖçÂáÜÁªìÊûú„ÄÇÊú¨Áî≥ËØ∑Âà©Áî®‰ΩéÂàÜËæ®ÁéáÁΩëÊ†ºÊ®°ÊùøËøõË°åÈÖçÂáÜËÉΩÊâæÂà∞‰∏Ä‰∏™Â§ßËá¥ÁöÑÈÖçÂáÜ‰ΩçÁΩÆÂå∫ÂüüÔºå‰ªéËÄå‰ΩøÂæóÂú®‰ΩøÁî®È´òÂàÜËæ®ÁéáÁΩëÊ†ºÊ®°ÊùøËøõË°åÈÖçÂáÜÊó∂Ôºå‰∏çËá≥‰∫é‰ºòÂåñÂà∞ÂÖ∂ÂÆÉÁöÑÂ±ÄÈÉ®ÊûÅÂÄºÁÇπ‰∏äÔºåÂà©Áî®È´òÂàÜËæ®ÁéáÁΩëÊ†ºÊ®°ÊùøËøõË°åÈÖçÂáÜÔºåÂèØÂü∫‰∫éÂèÇËÄÉÂÄºÊéíÈô§‰∏Ä‰∫õÂÅèÁ¶ªÂèÇËÄÉÂÄºËæÉÂ§ßÁöÑËÆ°ÁÆóÁªìÊûúÔºåÊúÄÁªàÂæóÂà∞Á≤æÁ°ÆÁöÑÈÖçÂáÜÁªìÊûú„ÄÇ </div></p>
    </tr>
   </tbody></table>
   <p class="margin-small">&nbsp;</p>







      </section>
      <footer>
        <p><small>Hosted on GitHub Pages </small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
